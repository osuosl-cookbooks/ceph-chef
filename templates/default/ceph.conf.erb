#######################################################################
# Maintained by ceph cookbook
# Please do not modify by hand. Changes will get overridden.
# Author: Hans Chris Jones
#######################################################################

[global]
# This value must be set before the mon.rb recipe is called. Do this in your own recipe where you set your owner
# variables. For an example, see ceph-chef/recipes/ceph-mon.rb
fsid = <%= @fsid_secret %>
<% if !node['ceph']['keyring']['global'].empty? %>
keyring = <%= node['ceph']['keyring']['global'] %>
<% end -%>
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
# Example of default cephx settings
# cephx require signatures = true
# cephx cluster require signatures = true
# cephx service require signatures = false
# cephx sign messages = true
# Note: mon host (required) and mon initial members (optional) should be in global section in addition
# to the cluster and public network options since they are all critical to every node.
# List of all of the monitor nodes in the given cluster
<% #Check ALL of the important vars before modifying them just in case their values are empty or nil. -%>
<% if !@mon_addresses.nil? && !@mon_addresses.empty? -%>
mon host = <%= @mon_addresses.sort.join(', ') %>
<% end -%>
# Suppress warning of too many pgs
mon pg warn max per osd = 0
max open files = 131072
<% if !node['ceph']['network']['cluster']['cidr'].nil? && !node['ceph']['network']['cluster']['cidr'].empty? %>
cluster network = <%= @node['ceph']['network']['cluster']['cidr'].join(',') %>
<% end -%>
<% if !node['ceph']['network']['public']['cidr'].nil? && !node['ceph']['network']['public']['cidr'].empty? %>
public network = <%= @node['ceph']['network']['public']['cidr'].join(',') %>
<% end -%>
<% if !node['ceph']['config']['global'].nil? -%>
# This is very flexible section. You can add more options OR override options from above simply by
# specifying the values in your wrapper cookbook or your "chef-repo". If you override values then
# you may see a warning in the logs letting you know you have overridden.
<% node['ceph']['config']['global'].sort.each do |k, v| %>
<%= k %> = <%= v %>
<% end %>
<% end -%>
<% if !node['ceph']['config']['rebalance'].nil? and !node['ceph']['config']['rebalance'].empty? %>
paxos propose interval = 60
<% end %>
<% if node['ceph']['experimental']['enable'] %>
enable experimental unrecoverable data corrupting features: <%= @node['ceph']['experimental']['features'].join(',') %>
<% end %>

<% if @is_osd -%>
[osd]
<% if !node['ceph']['keyring']['osd'].empty? %>
keyring = <%= node['ceph']['keyring']['osd'] %>
<% end -%>
<% # If the 'config''osd' key/value pairs are nil then use these values as defaults other wise be SURE to have at least these defined in your custom key/value OSD array -%>
<% if node['ceph']['config']['osd'].nil? -%>
# Set the default values here if no values provided to override them
# Need xattr use omap = true for RGW
osd map dedup = true
osd max write size = 256
osd disk threads = 1
osd client message size cap = 1073741824
filestore xattr use omap = true
filestore merge threshold = 40
filestore split multiple = 8
filestore op threads = 32
filestore min sync interval = 10
filestore max sync interval = 15
filestore queue max ops = 2500
filestore queue max bytes = 10485760
filestore queue committing max ops = 5000
filestore queue committing max bytes = 10485760000
osd journal size = <%= node['ceph']['osd']['journal']['size'] %>
journal max write bytes = 1073714824
journal max write entries = 10000
journal queue max ops = 50000
journal queue max bytes = 10485760000
journal block align = true
journal dio = true
journal aio = true
osd mkfs type = xfs
#osd mount options xfs = rw,noexec,nodev,noatime,nodiratime,nobarrier
osd mount options xfs = rw,noatime,inode64,logbufs=8,logbsize=256k,allocsize=4M
# Example: osd mkfs options xfs = -f -i size=2048
# Example: osd mount options xfs = noexec,nodev,noatime,nodiratime,barrier=0,discard
# IMPORTANT: If you modify the crush map with the automation then uncomment the line below (osd crush update ...)
# otherwise the crush map will not get created correctly and the PGs will stay in inactive/unclean state.
# In a production system it's good to set this value to 'false' and modify the crush map to fit your environment.
<% if node['ceph']['osd']['crush']['update'] %>
osd crush update on start = <%= node['ceph']['osd']['crush']['update_on_start'] %>
<% end %>
#You can change the replica count via config or cli
osd pool default size = <%= node['ceph']['osd']['size']['max'] %>
osd pool default min size = <%= node['ceph']['osd']['size']['min'] %>
osd pool default pg num = <%= node['ceph']['pools']['pgs']['num'] %>
osd pool default pgp num = <%= node['ceph']['pools']['pgs']['num'] %>
# Default is 0 for default crush rule.
# Example: osd pool default crush rule = <%= node['ceph']['pools']['crush']['rule'] %>
# Host 1, Rack 3 - Default is 1. This can have an impact on the crushmap if not the default.
osd crush chooseleaf type = <%= node['ceph']['osd']['crush']['chooseleaf_type'] %>
# Note: All of these values can be overridden in you wrapper cookbook or "chef-repo" project
osd recovery op priority = <%= node['ceph']['tuning']['osd_recovery_op_priority'] %>
osd recovery max active = <%= node['ceph']['tuning']['osd_recovery_max_active'] %>
osd max backfills = <%= node['ceph']['tuning']['osd_max_backfills'] %>
osd op threads = <%= node['ceph']['tuning']['osd_op_threads'] %>
osd disk thread ioprio priority = <%= node['ceph']['system']['scheduler']['device']['ceph']['priority'] %>
osd disk thread ioprio class = <%= node['ceph']['system']['scheduler']['device']['ceph']['class'] %>
<% end -%>
<% if !node['ceph']['config']['rebalance'].nil? and !node['ceph']['config']['rebalance'].empty? %>
# These two max active and max single work together
osd recovery max active = 1
osd recovery max single start = 1
osd recovery op priority = 1
osd recovery max chunk = 1048576
osd recovery threads = 1
osd max backfills  = 2
osd backfill scan min = 8
osd backfill scan max = 64
osd max scrubs = 1
osd scrub sleep = .1
osd scrub chunk min = 1
osd scrub chunk max = 5
osd deep scrub stride = 1048576
osd scrub begin hour = 23
osd scrub end hour = 6
osd op threads = 8
osd recovery op priority = 1
osd mon report interval min = 30
<% end %>
<% if !node['ceph']['config']['osd'].nil? -%>
<% node['ceph']['config']['osd'].sort.each do |k, v| %>
<%= k %> = <%= v %>
<% end %>
<% end -%>
<% end -%>

<% if @is_mon -%>
[mon]
<% if !node['ceph']['keyring']['mon'].empty? %>
keyring = <%= node['ceph']['keyring']['mon'] %>
<% end -%>
# Change the allow pool delete to false once you have everything where you want it. It keeps someone from
# accidently deleting a pool!
mon_allow_pool_delete = true
<% if !node['ceph']['config']['mon'].nil? -%>
<% node['ceph']['config']['mon'].sort.each do |k, v| %>
<%= k %> = <%= v %>
<% end %>
<% end -%>
<% end -%>

<% if !node['ceph']['config']['mds'].nil? -%>
# NOTE: Only needed for CephFS
[mds]
<% if !node['ceph']['keyring']['mds'].empty? %>
keyring = <%= node['ceph']['keyring']['mds'] %>
<% end -%>
mds cache size = 250000
<% node['ceph']['config']['mds'].sort.each do |key, value| -%>
<%= key %> = <%= value %>
<% end -%>
<% end -%>

<% if @is_rbd || @is_admin || @is_rgw || @is_rest_api -%>
# Any items in the 'client' section will apply to all sub-client sections unless overridden in the sub-client section.
[client]
<% if @is_rbd -%>
rbd cache = true
rbd cache writethrough until flush = true
rbd concurrent management ops = 10
rbd cache size = 67108864
rbd cache max dirty = 50331648
rbd cache target dirty = 33554432
rbd cache max dirty age = 2
rbd default format = 2
log file = /var/log/qemu/qemu-guest-$pid.log
# NOTE: If using RBD and OpenStack then you can add two additional sub-client for Cinder and Glance below this one.
<% end -%>
# admin socket must be writable by QEMU and allowed by SELinux or AppArmor
# admin socket = /var/run/ceph/$cluster-$type.$id.$pid.$cctid.asok
# admin socket = /var/run/ceph/$cluster-$type.$id.$pid.asok
# The `ceph daemon` is not as flexible as `ceph --admin-daemon` so naming admin socket is important.
admin socket = /var/run/ceph/$cluster-$type.$id.asok
<% end -%>

<% if @is_admin || @is_rgw -%>
# NOTE: You can null out the 'admin socket' below if you wish. If present, when running ceph -s without being 'sudo'
# you will receive an 'unable to bind error msg'.
[client.admin]
<% if !node['ceph']['keyring']['adm'].empty? %>
keyring = <%= node['ceph']['keyring']['adm'] %>
<% end -%>
admin socket =
log file =
<% end -%>

<% if @is_rgw -%>
<% if @is_federated -%>
<% node['ceph']['pools']['radosgw']['federated_zone_instances'].each do |inst| %>
[client.radosgw.<%= inst['region'] %>-<%= inst['name'] %>]
host = <%= node['hostname'] %>
rgw opstate ratelimit = 0
<% if node['ceph']['pools']['radosgw']['federated_enable_regions_zones'] %>
 <% if node['ceph']['pools']['radosgw']['federated_multisite_replication'] %>
rgw region = <%= inst['region'] %>
 <% else %>
rgw region = <%= inst['region'] %>-<%= inst['name'] %>
 <% end %>
<% if node['ceph']['version'] == 'hammer' %>
 <% if node['ceph']['pools']['radosgw']['federated_multisite_replication'] %>
rgw region root pool = .<%= inst['region'] %>.rgw.root
 <% else %>
rgw region root pool = .<%= inst['region'] %>-<%= inst['name'] %>.rgw.root
 <% end %>
<% else %>
 <% if node['ceph']['pools']['radosgw']['federated_multisite_replication'] %>
rgw zonegroup root pool = .<%= inst['region'] %>.rgw.root
 <% else %>
rgw zonegroup root pool = .<%= inst['region'] %>-<%= inst['name'] %>.rgw.root
 <% end %>
<% end %>
rgw zone = <%= inst['region'] %>-<%= inst['name'] %>
rgw zone root pool = .<%= inst['region'] %>-<%= inst['name'] %>.rgw.root
<% end %>
<% if !node['ceph']['keyring']['rgw'].empty? %>
<% if node['ceph']['pools']['radosgw']['federated_multisite_replication'] %>
keyring = <%= node['ceph']['keyring']['rgw'] %>.keyring
<% else %>
keyring = <%= node['ceph']['keyring']['rgw'] %>.<%= inst['region'] %>-<%= inst['name'] %>.keyring
<% end %>
<% end -%>
admin socket = /var/run/ceph/$cluster-$type.radosgw.<%= inst['region'] %>-<%= inst['name'] %>.asok
<% if inst['rgw_thread_pool'] %>
rgw thread pool size = <%= inst['rgw_thread_pool'] %>
<% else %>
rgw thread pool size = <%= node['ceph']['radosgw']['rgw_thread_pool'] %>
<% end %>
<% if inst['handles'] %>
rgw num rados handles = <%= inst['handles'] %>
<% else %>
rgw num rados handles = <%= node['ceph']['radosgw']['rgw_num_rados_handles'] %>
<% end %>
rgw enable ops log = <%= node['ceph']['radosgw']['logs']['ops']['enable'] %>
rgw enable usage log = <%= node['ceph']['radosgw']['logs']['usage']['enable'] %>
# For GC Max Objects: Use Prime numbers and larger ones if your cluster is large (i.e., 5003 or higher for a multi PB size)
rgw gc max objects = <%= node['ceph']['radosgw']['gc']['max_objects'] %>
rgw gc obj min wait = <%= node['ceph']['radosgw']['gc']['obj_min_wait'] %>
rgw gc processor max time = <%= node['ceph']['radosgw']['gc']['processor_max_time'] %>
rgw gc processor period = <%= node['ceph']['radosgw']['gc']['processor_period'] %>
pid file = /var/run/ceph/$name.<%= inst['region'] %>-<%= inst['name'] %>.pid
# Increased to 1 to log HTTP return codes - http://tracker.ceph.com/issues/12432
<% if node['ceph']['radosgw']['debug']['logs']['enable'] %>
<% if inst['threads'] %>
rgw frontends = civetweb port=<%= inst['port'] %> num_threads=<%= inst['threads'] %> error_log_file=<%= node['ceph']['radosgw']['civetweb_error_log_file'] %>.<%= inst['name'] %>.log access_log_file=<%= node['ceph']['radosgw']['civetweb_access_log_file'] %>.<%= inst['name'] %>.log
<% else %>
rgw frontends = civetweb port=<%= inst['port'] %> num_threads=<%= node['ceph']['radosgw']['civetweb_num_threads'] %> error_log_file=<%= node['ceph']['radosgw']['civetweb_error_log_file'] %>.<%= inst['name'] %>.log access_log_file=<%= node['ceph']['radosgw']['civetweb_access_log_file'] %>.<%= inst['name'] %>.log
<% end %>
log file = /var/log/radosgw/$cluster.client.radosgw.<%= inst['region'] %>-<%= inst['name'] %>.log
debug rgw = 1/5
<% else %>
<% if inst['threads'] %>
rgw frontends = civetweb port=<%= inst['port'] %> num_threads=<%= inst['threads'] %> error_log_file=<%= node['ceph']['radosgw']['civetweb_error_log_file'] %>.<%= inst['name'] %>.log
<% else %>
rgw frontends = civetweb port=<%= inst['port'] %> num_threads=<%= node['ceph']['radosgw']['civetweb_num_threads'] %> error_log_file=<%= node['ceph']['radosgw']['civetweb_error_log_file'] %>.<%= inst['name'] %>.log
<% end %>
debug default=0/0
debug lockdep=0/0
debug context=0/0
debug crush=0/0
debug buffer=0/0
debug timer=0/0
debug filer=0/0
debug objecter=0/0
debug rados=0/0
debug journaler=0/0
debug objectcacher=0/0
debug client=0/0
debug optracker=0/0
debug objclass=0/0
debug ms=0/0
debug tp=0/0
debug auth=0/0
debug finisher=0/0
debug heartbeatmap=0/0
debug perfcounter=0/0
debug rgw=0/0
debug asok=0/0
debug throttle=0/0
debug civetweb=0/0
<% end %>
rgw dns name = <%= inst['url'] %>
rgw resolve cname = True
<% if node['ceph']['radosgw']['keystone']['auth'] %>
rgw keystone url = <%= node['ceph']['radosgw']['keystone']['admin']['url'] %>:<%= node['ceph']['radosgw']['keystone']['admin']['port'] %>
rgw keystone admin token = <%= node['ceph']['radosgw']['keystone']['admin']['token'] %>
rgw keystone accepted roles = <%= node['ceph']['radosgw']['keystone']['accepted_roles'] %>
rgw keystone token cache size = <%= node['ceph']['radosgw']['keystone']['token_cache_size'] %>
rgw keystone revocation interval = <%= node['ceph']['radosgw']['keystone']['revocation_interval'] %>
rgw s3 auth use keystone = true
<% end %>
<% if !node['ceph']['config']['radosgw'].nil? -%>
<% node['ceph']['config']['radosgw'].sort.each do |k, v| %>
<%= k %> = <%= v %>
<% end %>
<% end -%>

<% end %>
<% else %>
[client.radosgw.gateway]
host = <%= node['hostname'] %>
rgw opstate ratelimit = 0
<% if !node['ceph']['keyring']['rgw'].empty? %>
keyring = <%= node['ceph']['keyring']['rgw'] %>.keyring
<% end -%>
admin socket = /var/run/ceph/$cluster-$type.radosgw.gateway.asok
rgw thread pool size = <%= node['ceph']['radosgw']['rgw_thread_pool'] %>
rgw num rados handles = <%= node['ceph']['radosgw']['rgw_num_rados_handles'] %>
rgw enable ops log = <%= node['ceph']['radosgw']['logs']['ops']['enable'] %>
rgw enable usage log = <%= node['ceph']['radosgw']['logs']['usage']['enable'] %>
# For GC Max Objects: Use Prime numbers and larger ones if your cluster is large (i.e., 5003 or higher for a multi PB size)
rgw gc max objects = <%= node['ceph']['radosgw']['gc']['max_objects'] %>
rgw gc obj min wait = <%= node['ceph']['radosgw']['gc']['obj_min_wait'] %>
rgw gc processor max time = <%= node['ceph']['radosgw']['gc']['processor_max_time'] %>
rgw gc processor period = <%= node['ceph']['radosgw']['gc']['processor_period'] %>
pid file = /var/run/ceph/$name.pid
# Increased to 1 to log HTTP return codes - http://tracker.ceph.com/issues/12432
<% if node['ceph']['radosgw']['debug']['logs']['enable'] %>
rgw frontends = civetweb port=<%= node['ceph']['radosgw']['port'] %> num_threads=<%= node['ceph']['radosgw']['civetweb_num_threads'] %> error_log_file=<%= node['ceph']['radosgw']['civetweb_error_log_file'] %>.log access_log_file=<%= node['ceph']['radosgw']['civetweb_access_log_file'] %>.log
log file = /var/log/radosgw/$cluster.client.radosgw.log
debug rgw = 1/5
<% else %>
rgw frontends = civetweb port=<%= node['ceph']['radosgw']['port'] %> num_threads=<%= node['ceph']['radosgw']['civetweb_num_threads'] %> error_log_file=<%= node['ceph']['radosgw']['civetweb_error_log_file'] %>.log
debug default=0/0
debug lockdep=0/0
debug context=0/0
debug crush=0/0
debug buffer=0/0
debug timer=0/0
debug filer=0/0
debug objecter=0/0
debug rados=0/0
debug journaler=0/0
debug objectcacher=0/0
debug client=0/0
debug optracker=0/0
debug objclass=0/0
debug ms=0/0
debug tp=0/0
debug auth=0/0
debug finisher=0/0
debug heartbeatmap=0/0
debug perfcounter=0/0
debug rgw=0/0
debug asok=0/0
debug throttle=0/0
debug civetweb=0/0
<% end %>
rgw dns name = <%= node['ceph']['radosgw']['default_url'] %>
rgw resolve cname = True
<% if node['ceph']['radosgw']['keystone']['auth'] %>
rgw keystone url = <%= node['ceph']['radosgw']['keystone']['admin']['url'] %>:<%= node['ceph']['radosgw']['keystone']['admin']['port'] %>
rgw keystone admin token = <%= node['ceph']['radosgw']['keystone']['admin']['token'] %>
rgw keystone accepted roles = <%= node['ceph']['radosgw']['keystone']['accepted_roles'] %>
rgw keystone token cache size = <%= node['ceph']['radosgw']['keystone']['token_cache_size'] %>
rgw keystone revocation interval = <%= node['ceph']['radosgw']['keystone']['revocation_interval'] %>
rgw s3 auth use keystone = true
<% end %>
<% if !node['ceph']['config']['radosgw'].nil? -%>
<% node['ceph']['config']['radosgw'].sort.each do |k, v| %>
<%= k %> = <%= v %>
<% end %>
<% end -%>
<% end -%>
<% end -%>

<% if @is_rest_api -%>
[client.restapi]
public addr = <%= node['ceph']['restapi']['ip'] %>:<%= node['ceph']['restapi']['port'] %>
<% if !node['ceph']['keyring']['res'].empty? %>
keyring = <%= node['ceph']['keyring']['res'] %>.keyring
<% end -%>
admin socket = /var/run/ceph/$cluster-$type.restapi.asok
restapi base url = <%= node['ceph']['restapi']['base_url'] %>
log file = /var/log/ceph/$cluster.client.restapi.log
<% if !node['ceph']['config']['restapi'].nil? -%>
<% node['ceph']['config']['restapi'].sort.each do |k, v| %>
<%= k %> = <%= v %>
<% end %>
<% end -%>
<% end -%>

<% node['ceph']['config-sections'].sort.each do |name, sect| %>
[<%= name %>]
<% sect.sort.each do |k, v| %>
<%= k %> = <%= v %>
<% end %>
<% end %>
